package com.ketul.kafka.stream.transform.stateful;

import com.ketul.kafka.message.AccountDetails;
import com.ketul.kafka.message.Customer;
import com.ketul.kafka.message.TransactionStatus;
import com.ketul.kafka.serde.CustomerDeserializer;
import com.ketul.kafka.serde.CustomerSerializer;
import com.ketul.kafka.serde.TransactionStatusDeserializer;
import com.ketul.kafka.serde.TransactionStatusSerializer;
import com.ketul.kafka.utils.StreamConstants;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.KeyValueStore;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

/**
 * Topic -> Stream -> Filter -> FlatMap -> GroupByKey -> Aggregate -> Foreach to print stream data
 * <p>
 * 1. Create input and output topic
 * bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic customer-input --replication-factor 3 --partitions 3
 * bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic customer-output --replication-factor 3 --partitions 3
 * <p>
 * 2. Start Customer Producer
 * <p>
 * 3. Start this stream. You can also start multiple instances of stream
 */
public class StreamAggregation {
    private static final Logger LOGGER = LoggerFactory.getLogger(StreamAggregation.class);

    public static void main(String[] args) {
        Properties properties = getStreamProperties();
        Topology topology = createTopology();
        LOGGER.info(topology.describe().toString());
        KafkaStreams streams = new KafkaStreams(topology, properties);
        streams.start();
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }

    private static Topology createTopology() {
        StreamsBuilder builder = new StreamsBuilder();
        Serde<Customer> customerSerdes = Serdes.serdeFrom(new CustomerSerializer(), new CustomerDeserializer());
        KGroupedStream<String, Customer> kGroupedStream = builder.stream(StreamConstants.INPUT_TOPIC,
                // Explicitly declaring serdes.
                Consumed.with(
                        Serdes.String(),
                        customerSerdes // Custom sedes
                ))
                .filter(((customerId, customer) -> customer.getAge() >= 25))
                .filterNot(((customerId, customer) -> customerId.startsWith("u")))
                /*
                This will mark stream data for re-partitioning. Applying group by or join after this triggers re-partitioning.
                That is the reason flatMapValues is preferred over flatMap.
                 */
                .flatMap(((customerId, customer) -> {
                    List<KeyValue<String, Customer>> customerList = new ArrayList<>();
                    customerList.add(KeyValue.pair(customerId.toLowerCase(), customer));
                    customerList.add(KeyValue.pair(customerId.toUpperCase(), customer));
                    return customerList;
                }))
                /*
                * groupByKey will trigger re-partition as stream is already marked for it by flatMap.
                * It will not trigger repartition if stream is not marked for it.
                *
                * It groups the record with existing key
                *
                *
                 */
                .groupByKey(
                        Grouped.with(
                                Serdes.String(),
                                customerSerdes
                        )
                );

        /*
        This calculates total amount withdrawn and deposited based on total last balance generated by producer
         */
        KTable<String, TransactionStatus> aggregatedBalanceTable = kGroupedStream.aggregate(
                () -> new TransactionStatus( 0.0f, 0.0f, 0.0f), // Initializing aggregated transaction status
                (customerId, customer, aggregatedTransactionStatus) -> {

                    LOGGER.info("Aggregated transaction status : {} for {}", aggregatedTransactionStatus.toString(), customerId);
                    LOGGER.info("New customer details : {} for {}", customer.toString(), customerId);
                    AccountDetails newAccountDetails = customer.getAccountDetails();
                    float amountWithdrawn = aggregatedTransactionStatus.getAmountWithdrawn();
                    float amountDeposited = aggregatedTransactionStatus.getAmountDeposited();
                    float newPreviousTransaction = customer.getAccountDetails().getLastBankBalance();
                    if(newAccountDetails.getLastBankBalance() > aggregatedTransactionStatus.getPreviousBalance()) {
                        amountDeposited += newAccountDetails.getLastBankBalance() - aggregatedTransactionStatus.getPreviousBalance();
                    } else if(newAccountDetails.getLastBankBalance() < aggregatedTransactionStatus.getPreviousBalance()) {
                        amountWithdrawn += aggregatedTransactionStatus.getPreviousBalance() - newAccountDetails.getLastBankBalance();
                    }
                    TransactionStatus status = new TransactionStatus(newPreviousTransaction, amountWithdrawn, amountDeposited);
                    LOGGER.info("New transaction status : {} for {}", status.toString(), customerId);
                    return status;
                },
                Materialized.<String, TransactionStatus, KeyValueStore<Bytes, byte[]>>as("transaction-status-store")
                        .withValueSerde(Serdes.serdeFrom(new TransactionStatusSerializer(), new TransactionStatusDeserializer()))
                        .withKeySerde(Serdes.String())
        );

        aggregatedBalanceTable.toStream().foreach((customerId, transactionStatus) -> LOGGER.info(String.format("%s => %s", customerId, transactionStatus.toString())));
        return builder.build();
    }

    private static Properties getStreamProperties() {
        Properties properties = new Properties();
        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, StreamConstants.APPLICATION_ID);
        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, StreamConstants.BOOTSTRAP_SERVERS);
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, StreamConstants.AUTO_OFFSET_RESET_EARLIEST);
        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
        return properties;
    }
}
